{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58242d74-900a-4a82-8225-2a999ba46bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import re\n",
    "import gc\n",
    "\n",
    "from PIL import Image\n",
    "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27cf3ad-4fec-4922-8114-17524c9b480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b17fbf-2186-4bef-9ad1-2992bcf26fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cuda'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9991a9-1648-4a79-a4d7-4952a25881b3",
   "metadata": {},
   "source": [
    "## SAM2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad28fc-cb1e-4726-ab79-fdd959b58c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap('tab10')\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    ax.axis('off')\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c7563-3d13-43d9-8f98-69ce5a89e278",
   "metadata": {},
   "source": [
    "## Extract Video Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b1d98b-8762-4304-8cb7-346611ff8768",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames_dir = 'video_frames'\n",
    "os.makedirs(video_frames_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb689c4-633e-40ac-8fd9-1142a39b46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7dcaa-bafe-4cbf-b0f6-e9116ffe1773",
   "metadata": {},
   "source": [
    "For your custom videos, you can extract their JPEG frames using ffmpeg (https://ffmpeg.org/) as follows:\n",
    "```\n",
    "ffmpeg -i <your_video>.mp4 -q:v 2 -start_number 0 <output_dir>/'%05d.jpg'\n",
    "```\n",
    "where `-q:v` generates high-quality JPEG frames and `-start_number 0` asks ffmpeg to start the JPEG file from `00000.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66a562-1203-4b2b-b85a-d4634f71cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i ../demo_data/video_1.mp4 -q:v 2 -start_number 0 video_frames/'%05d.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b8e6d-4ee5-4425-87ca-b7f6928052a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frames and visualize the first one.\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_frames_dir)\n",
    "    if os.path.splitext(p)[-1] in ['.jpg', '.jpeg', '.JPG', '.JPEG']\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# take a look the first video frame\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_frames_dir, frame_names[frame_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce135d9-a136-4c35-8c68-1b87efa3634e",
   "metadata": {},
   "source": [
    "## Get the Molmo Points and Delete Model from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3af64-e268-4844-b255-ccfb41125215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Molmo model and processor.\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# load the processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    'allenai/MolmoE-1B-0924',\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype='auto'\n",
    ")\n",
    "\n",
    "# load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'allenai/MolmoE-1B-0924',\n",
    "    trust_remote_code=True,\n",
    "    offload_folder='offload',\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2cb47-9506-441f-b0fe-4e4bc2b0edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_point_and_show(image_path=None, points=None):\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    for point in points:\n",
    "        image = cv2.circle(\n",
    "            image, \n",
    "            (point[0], point[1]), \n",
    "            radius=5, \n",
    "            color=(0, 255, 0), \n",
    "            thickness=5,\n",
    "            lineType=cv2.LINE_AA\n",
    "        )\n",
    "\n",
    "    plt.imshow(image[..., ::-1])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_coords(output_string, image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w, _ = image.shape\n",
    "    \n",
    "    if 'points' in output_string:\n",
    "        # Handle multiple coordinates\n",
    "        matches = re.findall(r'(x\\d+)=\"([\\d.]+)\" (y\\d+)=\"([\\d.]+)\"', output_string)\n",
    "        coordinates = [(int(float(x_val)/100*w), int(float(y_val)/100*h)) for _, x_val, _, y_val in matches]\n",
    "    else:\n",
    "        # Handle single coordinate\n",
    "        match = re.search(r'x=\"([\\d.]+)\" y=\"([\\d.]+)\"', output_string)\n",
    "        if match:\n",
    "            coordinates = [(int(float(match.group(1))/100*w), int(float(match.group(2))/100*h))]\n",
    "            \n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b9b65-2e5c-49ce-b49f-bea6491c6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(image_path=None, prompt='Describe this image.'):\n",
    "    # process the image and text\n",
    "    if image_path:\n",
    "        inputs = processor.process(\n",
    "            images=[Image.open(image_path)],\n",
    "            text=prompt\n",
    "        )\n",
    "    else:\n",
    "        inputs = processor.process(\n",
    "            images=[Image.open(requests.get('https://picsum.photos/id/237/536/354', stream=True).raw)],\n",
    "            text=prompt\n",
    "        )\n",
    "\n",
    "    # move inputs to the correct device and make a batch of size 1\n",
    "    inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n",
    "\n",
    "    # generate output; maximum 200 new tokens; stop generation when <|endoftext|> is generated\n",
    "    output = model.generate_from_batch(\n",
    "        inputs,\n",
    "        GenerationConfig(max_new_tokens=200, stop_strings='<|endoftext|>'),\n",
    "        tokenizer=processor.tokenizer\n",
    "    )\n",
    "\n",
    "    # only get generated tokens; decode them to text\n",
    "    generated_tokens = output[0,inputs['input_ids'].size(1):]\n",
    "    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # print the generated text\n",
    "    print(generated_text)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87230595-28d8-4993-96d0-63954d318f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the first frame to get the coordinates.\n",
    "image_path = 'video_frames/00000.jpg'\n",
    "\n",
    "outputs = get_output(image_path=image_path, prompt=\"Point to the main player's shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a971ebb3-bb8e-4263-9727-edb508ce668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del processor, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332971f-1c54-4886-bee8-3c9263ed4e2e",
   "metadata": {},
   "source": [
    "## Initialize SAM2 Inference State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c4e9e-fac1-4539-a59c-2ecca6011ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SAM2VideoPredictor.from_pretrained(\n",
    "    'facebook/sam2.1-hiera-tiny', device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d7979-f35a-46b7-bead-7198aa856df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode(), torch.autocast(device, dtype=torch.bfloat16):\n",
    "inference_state = predictor.init_state(video_path=video_frames_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3e2ef-70eb-44e6-8042-46c6f3183c86",
   "metadata": {},
   "source": [
    "## Segment and Track and Object with Manual Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b5058-0c9a-4e43-925a-1518893396d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_point1 = 310\n",
    "object_point2 = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1752f41-7763-4cbf-b129-82d4598abbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frame = Image.open(os.path.join(video_frames_dir, frame_names[frame_idx]))\n",
    "w, h = sample_frame.size\n",
    "print(w, h)\n",
    "plt.imshow(sample_frame)\n",
    "plt.plot(object_point1, object_point2, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba23c8-fb96-4f14-a3b7-56e8e6ad5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0 # Frame index to interact/start with.\n",
    "ann_object_id = 1 # Give a unique object ID to the object, an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412ce95-9a4d-42f4-a473-35c5b92b4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the coordinate to track, here the ball.\n",
    "points = np.array([[object_point1, object_point2]], dtype=np.float32)\n",
    "# Add positive label, 1 to track. Negative labels, 0 do not track objects.\n",
    "labels = np.array([1], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a22c0-6449-4565-91eb-21c059575d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode(), torch.autocast(device, dtype=torch.bfloat16):\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_object_id,\n",
    "    points=points,\n",
    "    labels=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cba711-cd03-49a3-9b59-dee8bb92973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results.\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_frames_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43249a9b-64e5-4bcd-9697-325f9baf4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propgate the prompt to get masklet across the video.\n",
    "# Run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # `video_segments` contains the per-frame segmentation results\n",
    "max_frame_num_to_track = None\n",
    "# with torch.inference_mode(), torch.autocast(device, dtype=torch.bfloat16):\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(\n",
    "    inference_state, max_frame_num_to_track=max_frame_num_to_track\n",
    "):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c711b9-a0de-4c5f-8771-54e5d4610a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'video_out'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1e421-590d-4388-a1fb-d56dd3da14a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV VideoWriter\n",
    "codec = cv2.VideoWriter_fourcc(\"X\", \"V\", \"I\", \"D\")\n",
    "# codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(\n",
    "    f\"{output_dir}/output.avi\",\n",
    "    codec, 30,\n",
    "    (w, h)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa0ee7-580d-4669-88f8-7b004e31674d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize a few segmentation result frames.\n",
    "vis_frame_stride = 1\n",
    "plt.close('all')\n",
    "\n",
    "dpi = plt.rcParams['figure.dpi']\n",
    "\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    #### SAM visulization starts here ####\n",
    "    image = Image.open(os.path.join(video_frames_dir, frame_names[out_frame_idx]))\n",
    "\n",
    "    figsize = image.size[0] / dpi, image.size[1] / dpi\n",
    "    plt.figure(figsize=figsize)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, ax, obj_id=out_obj_id)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    #### SAM visulization ends here ####\n",
    "    \n",
    "    #### Converting to Numpy and saving video starts here ####\n",
    "    # Convert the Matplotlib plot to a NumPy array\n",
    "    canvas = FigureCanvas(fig)\n",
    "    canvas.draw()\n",
    "    \n",
    "    # Get the RGBA buffer from the figure\n",
    "    image_rgba = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8).reshape(h, w, 4)\n",
    "\n",
    "    # Convert ARGB to RGBA\n",
    "    image_rgba = np.roll(image_rgba, 3, axis=2)\n",
    "\n",
    "    # Convert RGBA to RGB by discarding the alpha channel\n",
    "    image_rgb = image_rgba[..., :3]\n",
    "\n",
    "    # Convert RGB to BGR for OpenCV\n",
    "    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Save the image using OpenCV\n",
    "    out.write(image_bgr)\n",
    "\n",
    "# Close the plot to free memory\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5acbb1d-bcaf-4c13-ad7b-59e09bf756f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b0efa-2ae4-4df6-8c08-749722deab19",
   "metadata": {},
   "source": [
    "## Use Molmo Coordinates for SAM Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cafdf-94f3-4054-9397-7de6859666f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = get_coords(outputs, image_path=image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183e868-e7a5-407f-bbc0-06c2324f4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c196444-38f4-4835-a760-ba66792c70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_point_and_show(image_path, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1e449-5b60-4eb6-876d-d469a61b1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_points = np.array(coords)\n",
    "input_labels = np.ones(len(input_points), dtype=np.int32)\n",
    "print(input_points, input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3bf7a-88d0-4cfd-8787-99d3913348a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add both the shoe points.\n",
    "for i in range(len(input_points)):\n",
    "    input_point = np.array([input_points[i]])\n",
    "    input_label = np.array([input_labels[i]])\n",
    "    ann_frame_idx = 0 # Frame index to interact/start with.\n",
    "    ann_object_id = i # Give a unique object ID to the object, an integer.\n",
    "\n",
    "    # with torch.inference_mode(), torch.autocast(device, dtype=torch.bfloat16):\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_object_id,\n",
    "        points=input_point,\n",
    "        labels=input_label\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46a2be-7519-407a-842b-add0f2fd024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the first frame only.\n",
    "# Visualize results.\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_frames_dir, frame_names[ann_frame_idx])))\n",
    "show_points(input_points, input_labels, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfdd9f5-7b30-4a3f-b3dc-48fe6226924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagate through the entire video.\n",
    "# Propgate the prompt to get masklet across the video.\n",
    "# Run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # `video_segments` contains the per-frame segmentation results\n",
    "max_frame_num_to_track = None\n",
    "# with torch.inference_mode(), torch.autocast(device, dtype=torch.bfloat16):\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(\n",
    "    inference_state, max_frame_num_to_track=max_frame_num_to_track\n",
    "):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5f5ad-b4be-435a-9ad8-135d31031fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'video_out'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833da64-d7ca-4a7b-a400-963a279efc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV VideoWriter\n",
    "codec = cv2.VideoWriter_fourcc(\"X\", \"V\", \"I\", \"D\")\n",
    "# codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(\n",
    "    f\"{output_dir}/molmo_points_output.avi\",\n",
    "    codec, 30,\n",
    "    (w, h)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128977d0-652f-480b-a558-8015bc60fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few segmentation result frames.\n",
    "vis_frame_stride = 1\n",
    "plt.close('all')\n",
    "\n",
    "dpi = plt.rcParams['figure.dpi']\n",
    "\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    #### SAM visulization starts here ####\n",
    "    image = Image.open(os.path.join(video_frames_dir, frame_names[out_frame_idx]))\n",
    "\n",
    "    figsize = image.size[0] / dpi, image.size[1] / dpi\n",
    "    plt.figure(figsize=figsize)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, ax, obj_id=out_obj_id)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    #### SAM visulization ends here ####\n",
    "    \n",
    "    #### Converting to Numpy and saving video starts here ####\n",
    "    # Convert the Matplotlib plot to a NumPy array\n",
    "    canvas = FigureCanvas(fig)\n",
    "    canvas.draw()\n",
    "    \n",
    "    # Get the RGBA buffer from the figure\n",
    "    image_rgba = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8).reshape(h, w, 4)\n",
    "\n",
    "    # Convert ARGB to RGBA\n",
    "    image_rgba = np.roll(image_rgba, 3, axis=2)\n",
    "\n",
    "    # Convert RGBA to RGB by discarding the alpha channel\n",
    "    image_rgb = image_rgba[..., :3]\n",
    "\n",
    "    # Convert RGB to BGR for OpenCV\n",
    "    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Save the image using OpenCV\n",
    "    out.write(image_bgr)\n",
    "\n",
    "# Close the plot to free memory\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bd62f-597b-44a7-b73b-5bb4ae9dee7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce59e92-8760-493a-9c79-1506c9051fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
