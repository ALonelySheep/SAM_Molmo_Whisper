{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be635707-d3dc-4a51-a210-d81dccbe6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c979ff-b6bd-4da5-afa7-eba6e7c83574",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69fc81-720e-4e26-9343-f6c4c5f672a4",
   "metadata": {},
   "source": [
    "## SAM2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c6d5cf-06cc-478c-b6dd-d87d94010990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for SAM2 segmentation map visualization.\n",
    "def show_mask(mask, plt, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([255/255, 40/255, 50/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    \n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask,cv2.RETR_EXTERNAL, \n",
    "            cv2.CHAIN_APPROX_NONE\n",
    "        )\n",
    "        # Try to smooth contours\n",
    "        contours = [\n",
    "            cv2.approxPolyDP(\n",
    "                contour, epsilon=0.01, closed=True\n",
    "            ) for contour in contours\n",
    "        ]\n",
    "        mask_image = cv2.drawContours(\n",
    "            mask_image, \n",
    "            contours, \n",
    "            -1, \n",
    "            (1, 0, 0, 1), \n",
    "            thickness=2\n",
    "        ) \n",
    "    plt.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(\n",
    "        pos_points[:, 0], \n",
    "        pos_points[:, 1], \n",
    "        color='green', \n",
    "        marker='.', \n",
    "        s=marker_size, \n",
    "        edgecolor='white', \n",
    "        linewidth=1.25\n",
    "    )\n",
    "    ax.scatter(\n",
    "        neg_points[:, 0], \n",
    "        neg_points[:, 1], \n",
    "        color='red', \n",
    "        marker='.', \n",
    "        s=marker_size, \n",
    "        edgecolor='white', \n",
    "        linewidth=1.25\n",
    "    )   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle(\n",
    "        (x0, y0), \n",
    "        w, \n",
    "        h, \n",
    "        edgecolor='green', \n",
    "        facecolor=(0, 0, 0, 0), \n",
    "        lw=2)\n",
    "    )    \n",
    "\n",
    "def show_masks(\n",
    "    image, \n",
    "    masks, \n",
    "    scores, \n",
    "    point_coords=None, \n",
    "    box_coords=None, \n",
    "    input_labels=None, \n",
    "    borders=True\n",
    "):\n",
    "    dpi = plt.rcParams['figure.dpi']\n",
    "    figsize = image.shape[1] / dpi, image.shape[0] / dpi\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image)\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        if i == 0:  # Only show the highest scoring mask.\n",
    "            show_mask(mask, plt, random_color=False, borders=borders)\n",
    "    if point_coords is not None:\n",
    "        assert input_labels is not None\n",
    "        show_points(point_coords, input_labels, plt.gca())\n",
    "    if box_coords is not None:\n",
    "        show_box(box_coords, plt.gca())\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('figure.png')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f74eff24-4749-40f9-84b5-b6716d131ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SAM2ImagePredictor.from_pretrained('facebook/sam2.1-hiera-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449251d-1587-499b-a202-5199cd22d30a",
   "metadata": {},
   "source": [
    "## Integrate Molmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "390b671e-4436-4abf-a240-9424c2831374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPTNeoXTokenizer'. \n",
      "The class this function is called from is 'GPT2TokenizerFast'.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallenai/MolmoE-1B-0924\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# load the model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mallenai/MolmoE-1B-0924\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moffload\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/transformers/modeling_utils.py:3900\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     keep_in_fp32_modules \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3900\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\n\u001b[1;32m   3902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3904\u001b[0m     \u001b[38;5;66;03m# We store the original dtype for quantized models as we cannot easily retrieve it\u001b[39;00m\n\u001b[1;32m   3905\u001b[0m     \u001b[38;5;66;03m# once the weights have been quantized\u001b[39;00m\n\u001b[1;32m   3906\u001b[0m     \u001b[38;5;66;03m# Note that once you have loaded a quantized model, you can't change its dtype so this will\u001b[39;00m\n\u001b[1;32m   3907\u001b[0m     \u001b[38;5;66;03m# remain a single source of truth\u001b[39;00m\n\u001b[1;32m   3908\u001b[0m     config\u001b[38;5;241m.\u001b[39m_pre_quantization_dtype \u001b[38;5;241m=\u001b[39m torch_dtype\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/transformers/quantizers/base.py:182\u001b[0m, in \u001b[0;36mHfQuantizer.preprocess_model\u001b[0;34m(self, model, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m model\u001b[38;5;241m.\u001b[39mis_quantized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m model\u001b[38;5;241m.\u001b[39mquantization_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mquant_method\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_model_before_weight_loading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:313\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer._process_model_before_weight_loading\u001b[0;34m(self, model, device_map, keep_in_fp32_modules, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    307\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want to offload some keys to `cpu` or `disk`, you need to set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llm_int8_enable_fp32_cpu_offload=True`. Note that these modules will not be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m converted to 8-bit but kept in 32-bit.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m         )\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\u001b[38;5;241m.\u001b[39mextend(keys_on_cpu)\n\u001b[0;32m--> 313\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mreplace_with_bnb_linear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodules_to_not_convert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules_to_not_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# TODO: consider bringing replace_with_bnb_linear() code from ..integrations/bitsandbyter.py to here\u001b[39;00m\n\u001b[1;32m    318\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization_config\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:260\u001b[0m, in \u001b[0;36mreplace_with_bnb_linear\u001b[0;34m(model, modules_to_not_convert, current_key_name, quantization_config)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03mA helper function to replace all `torch.nn.Linear` modules by `bnb.nn.Linear8bit` modules from the `bitsandbytes`\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mlibrary. This will enable running your models using mixed int8 precision as described by the paper `LLM.int8():\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m        storage and computation.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m modules_to_not_convert \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_head\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m modules_to_not_convert \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m modules_to_not_convert\n\u001b[0;32m--> 260\u001b[0m model, has_been_replaced \u001b[38;5;241m=\u001b[39m \u001b[43m_replace_with_bnb_linear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodules_to_not_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_key_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_been_replaced:\n\u001b[1;32m    265\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are loading your model in 8bit or 4bit but no linear modules were found in your model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please double check your model architecture, or submit an issue on github if you think this is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a bug.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:217\u001b[0m, in \u001b[0;36m_replace_with_bnb_linear\u001b[0;34m(model, modules_to_not_convert, current_key_name, quantization_config, has_been_replaced)\u001b[0m\n\u001b[1;32m    215\u001b[0m             model\u001b[38;5;241m.\u001b[39m_modules[name]\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(module\u001b[38;5;241m.\u001b[39mchildren())) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     _, has_been_replaced \u001b[38;5;241m=\u001b[39m \u001b[43m_replace_with_bnb_linear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodules_to_not_convert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_key_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_been_replaced\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_been_replaced\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Remove the last key for recursion\u001b[39;00m\n\u001b[1;32m    225\u001b[0m current_key_name\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:217\u001b[0m, in \u001b[0;36m_replace_with_bnb_linear\u001b[0;34m(model, modules_to_not_convert, current_key_name, quantization_config, has_been_replaced)\u001b[0m\n\u001b[1;32m    215\u001b[0m             model\u001b[38;5;241m.\u001b[39m_modules[name]\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(module\u001b[38;5;241m.\u001b[39mchildren())) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     _, has_been_replaced \u001b[38;5;241m=\u001b[39m \u001b[43m_replace_with_bnb_linear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodules_to_not_convert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_key_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_been_replaced\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_been_replaced\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Remove the last key for recursion\u001b[39;00m\n\u001b[1;32m    225\u001b[0m current_key_name\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "    \u001b[0;31m[... skipping similar frames: _replace_with_bnb_linear at line 217 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:217\u001b[0m, in \u001b[0;36m_replace_with_bnb_linear\u001b[0;34m(model, modules_to_not_convert, current_key_name, quantization_config, has_been_replaced)\u001b[0m\n\u001b[1;32m    215\u001b[0m             model\u001b[38;5;241m.\u001b[39m_modules[name]\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(module\u001b[38;5;241m.\u001b[39mchildren())) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     _, has_been_replaced \u001b[38;5;241m=\u001b[39m \u001b[43m_replace_with_bnb_linear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodules_to_not_convert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_key_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_been_replaced\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_been_replaced\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Remove the last key for recursion\u001b[39;00m\n\u001b[1;32m    225\u001b[0m current_key_name\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:202\u001b[0m, in \u001b[0;36m_replace_with_bnb_linear\u001b[0;34m(model, modules_to_not_convert, current_key_name, quantization_config, has_been_replaced)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         extra_kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    198\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_storage\u001b[39m\u001b[38;5;124m\"\u001b[39m: quantization_config\u001b[38;5;241m.\u001b[39mbnb_4bit_quant_storage}\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_storage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(signature(bnb\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear4bit)\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[1;32m    200\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    201\u001b[0m         )\n\u001b[0;32m--> 202\u001b[0m         model\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear4bit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompress_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m         has_been_replaced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Store the module class in case we need to transpose the weight later\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:396\u001b[0m, in \u001b[0;36mLinear4bit.__init__\u001b[0;34m(self, input_features, output_features, bias, compute_dtype, compress_statistics, quant_type, quant_storage, device)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    376\u001b[0m     input_features,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    384\u001b[0m ):\n\u001b[1;32m    385\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Initialize Linear4bit class.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m            Whether the linear class uses the bias term as well.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Params4bit(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m    399\u001b[0m         requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m         module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    404\u001b[0m     )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# self.persistent_buffers = []  # TODO consider as way to save quant state\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/nn/modules/linear.py:104\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/nn/modules/linear.py:110\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/nn/init.py:460\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    458\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/_decomp/decompositions.py:2639\u001b[0m, in \u001b[0;36muniform_\u001b[0;34m(self, low, high, generator)\u001b[0m\n\u001b[1;32m   2637\u001b[0m \u001b[38;5;129m@register_decomposition\u001b[39m(aten\u001b[38;5;241m.\u001b[39muniform_)\n\u001b[1;32m   2638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21muniform_\u001b[39m(\u001b[38;5;28mself\u001b[39m, low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 2639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_(\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/_prims_common/wrappers.py:266\u001b[0m, in \u001b[0;36mout_wrapper.<locals>._out_wrapper.<locals>._fn\u001b[0;34m(out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, is_out\u001b[38;5;241m=\u001b[39m(out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(result, TensorLike)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_tensor\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Tuple)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(out_names)\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# Naively you might expect this assert to be true, but\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# it's not:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# be a normal meta tensor, but this is perfectly\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# harmless.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/_decomp/decompositions.py:2627\u001b[0m, in \u001b[0;36muniform\u001b[0;34m(x, low, high, generator)\u001b[0m\n\u001b[1;32m   2619\u001b[0m \u001b[38;5;129m@register_decomposition\u001b[39m(aten\u001b[38;5;241m.\u001b[39muniform)\n\u001b[1;32m   2620\u001b[0m \u001b[38;5;129m@out_wrapper\u001b[39m()\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21muniform\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2625\u001b[0m     generator: Optional[torch\u001b[38;5;241m.\u001b[39mGenerator] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2626\u001b[0m ):\n\u001b[0;32m-> 2627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_uniform_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msym_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhigh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msym_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhigh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/_ops.py:667\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/_prims/__init__.py:299\u001b[0m, in \u001b[0;36m_make_prim.<locals>._backend_select_impl\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backend_select_impl\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 299\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeta\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m meta(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/_prims/__init__.py:2887\u001b[0m, in \u001b[0;36m_uniform_meta\u001b[0;34m(shape, low, high, dtype, device, generator)\u001b[0m\n\u001b[1;32m   2878\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_uniform_meta\u001b[39m(\n\u001b[1;32m   2879\u001b[0m     shape: ShapeType,\n\u001b[1;32m   2880\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2885\u001b[0m     generator: Optional[torch\u001b[38;5;241m.\u001b[39mGenerator] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2886\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorLikeType:\n\u001b[0;32m-> 2887\u001b[0m     strides \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_contiguous_strides_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TensorMeta(shape\u001b[38;5;241m=\u001b[39mshape, strides\u001b[38;5;241m=\u001b[39mstrides, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/_prims_common/__init__.py:1606\u001b[0m, in \u001b[0;36mmake_contiguous_strides_for\u001b[0;34m(shape, row_major)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(shape):\n\u001b[1;32m   1605\u001b[0m     strides\u001b[38;5;241m.\u001b[39mappend(multiplier)\n\u001b[0;32m-> 1606\u001b[0m     multiplier \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m l \u001b[38;5;28;01mif\u001b[39;00m is_nested_int(l) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msym_max\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1608\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(strides))\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;66;03m# batched_matrix_contiguous_strides from aten/src/ATen/native/LinearAlgebraUtils.h\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/__init__.py:669\u001b[0m, in \u001b[0;36msym_max\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03mSymInt-aware utility for max which avoids branching on a < b.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03mUnlike builtins.max(), this only works for int/float, and it always\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03mpromotes to float if any argument is float (unlike builtins.max, which\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03mwill faithfully preserve the type of the input argument).\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_torch_function, handle_torch_function\n\u001b[0;32m--> 669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_torch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(sym_max, (a, b), a, b)\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, (SymInt, SymFloat)):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load Molmo model and processor.\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# load the processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    'allenai/MolmoE-1B-0924',\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype='auto'\n",
    ")\n",
    "\n",
    "# load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'allenai/MolmoE-1B-0924',\n",
    "    trust_remote_code=True,\n",
    "    offload_folder='offload',\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797548b6-3094-4945-894c-b69cf9e0364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_point_and_show(image_path=None, points=None):\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    for point in points:\n",
    "        image = cv2.circle(\n",
    "            image, \n",
    "            (point[0], point[1]), \n",
    "            radius=5, \n",
    "            color=(0, 255, 0), \n",
    "            thickness=5,\n",
    "            lineType=cv2.LINE_AA\n",
    "        )\n",
    "\n",
    "    plt.imshow(image[..., ::-1])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_coords(output_string, image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w, _ = image.shape\n",
    "    \n",
    "    if 'points' in output_string:\n",
    "        # Handle multiple coordinates\n",
    "        matches = re.findall(r'(x\\d+)=\"([\\d.]+)\" (y\\d+)=\"([\\d.]+)\"', output_string)\n",
    "        coordinates = [(int(float(x_val)/100*w), int(float(y_val)/100*h)) for _, x_val, _, y_val in matches]\n",
    "    else:\n",
    "        # Handle single coordinate\n",
    "        match = re.search(r'x=\"([\\d.]+)\" y=\"([\\d.]+)\"', output_string)\n",
    "        if match:\n",
    "            coordinates = [(int(float(match.group(1))/100*w), int(float(match.group(2))/100*h))]\n",
    "            \n",
    "    return coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a13987-00d9-4800-b21f-8fd42958cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(image_path=None, prompt='Describe this image.'):\n",
    "    # process the image and text\n",
    "    if image_path:\n",
    "        inputs = processor.process(\n",
    "            images=[Image.open(image_path)],\n",
    "            text=prompt\n",
    "        )\n",
    "    else:\n",
    "        inputs = processor.process(\n",
    "            images=[Image.open(requests.get('https://picsum.photos/id/237/536/354', stream=True).raw)],\n",
    "            text=prompt\n",
    "        )\n",
    "\n",
    "    # move inputs to the correct device and make a batch of size 1\n",
    "    inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n",
    "\n",
    "    # generate output; maximum 200 new tokens; stop generation when <|endoftext|> is generated\n",
    "    output = model.generate_from_batch(\n",
    "        inputs,\n",
    "        GenerationConfig(max_new_tokens=200, stop_strings='<|endoftext|>'),\n",
    "        tokenizer=processor.tokenizer\n",
    "    )\n",
    "\n",
    "    # only get generated tokens; decode them to text\n",
    "    generated_tokens = output[0,inputs['input_ids'].size(1):]\n",
    "    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # print the generated text\n",
    "    print(generated_text)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c269aa-feaa-4177-95cf-728ab1cdada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../demo_data/image_4.jpg'\n",
    "\n",
    "outputs = get_output(image_path=image_path, prompt='Point to the giraffes and the woman.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdab4e4-a71c-4db7-be34-8fc2bdf52413",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = get_coords(outputs, image_path=image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731cee1-1878-40a0-b1f6-4caef93c5f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717040d3-ce9e-4adb-9b4b-3facc1d2a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_point_and_show(image_path, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52dc556-d8ee-4184-b579-f7ed6a4feda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_points = np.array(coords)\n",
    "input_labels = np.ones(len(input_points), dtype=np.int32)\n",
    "print(input_points, input_labels)\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.array(image)\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d370d-110b-4d93-bfcf-d15af9660777",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_points,\n",
    "    point_labels=input_labels,\n",
    "    multimask_output=True,\n",
    ")\n",
    "sorted_ind = np.argsort(scores)[::-1]\n",
    "masks = masks[sorted_ind]\n",
    "scores = scores[sorted_ind]\n",
    "logits = logits[sorted_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8b4a3-20e0-4637-9911-6fd5895df2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, point_coords=input_points, input_labels=input_labels, borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2813168-3333-473c-b071-fb2d4c5ebe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../demo_data/image_4.jpg'\n",
    "\n",
    "outputs = get_output(image_path=image_path, prompt='Point to all the giraffes and persons.')\n",
    "\n",
    "coords = get_coords(outputs, image_path=image_path)\n",
    "print(coords)\n",
    "\n",
    "draw_point_and_show(image_path, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6911a017-10bb-4eb3-a5e2-bac03d43f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_points = np.array(coords)\n",
    "input_labels = np.ones(len(input_points), dtype=np.int32)\n",
    "print(input_points, input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317aed2a-aeaf-4b89-8855-460c8a8d352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path).convert('RGB')\n",
    "image = np.array(image)\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c0d87-f0e3-4777-937e-ea5e63e7541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_points,\n",
    "    point_labels=input_labels,\n",
    "    multimask_output=True,\n",
    ")\n",
    "sorted_ind = np.argsort(scores)[::-1]\n",
    "masks = masks[sorted_ind]\n",
    "scores = scores[sorted_ind]\n",
    "logits = logits[sorted_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20123a4-56e3-44a7-a698-5931a953a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_masks(image, masks, scores, point_coords=input_points, input_labels=input_labels, borders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0639e1-cb0a-408e-8e5b-bfbf54331410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a6229-0bff-4852-ad5f-b0c6d071fe88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
